---
title: "Machine Learning Project"
author: "David Dessert"
date: "November 13, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction
The Machine Learning project will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. Each participate performs 5 separate *classe* (A - E) of exercises while measurements are recorded from these accelerometers. The goal is to identify the *classe* of exercise in 20 separate test cases.

## Methods
A total of nine models were trained with the training data set. *classe* accuracy determination using the test data (OOS Accuracy) identified the six best performing models. All six models predicted the *classe* on the validation set. The predictions from these six models voted on the correct *classe*. Comparison of the six individual model and voted accuracies were recorded, demonstrating that the voting method improved accuracy.  
Finally all six models predicted *classe* on the final test data and voted for the final predictions.

## Load necessary packages to execute
```{r warning=FALSE}
library(caret)
library(ggplot2)
library(randomForest)
```

Define a mode function that we'll use to vote for the final prediction.
```{r}
Mode <- function(x)
{
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
}
```

## Load and preprocess the data  
We have an overabundance of predictors in these data files. First we'll eliminate any predictors that cannot be of use to us.  
Load the test data file and mark any columns containing only data that are NA to be removed from the data sets. Also remove the two time columns, the row number (X), and the num_window columns.    
Load the training data and remove the same columns we removed in the test data file. The training file has extra rows that were used by the authors but have no use for us here.  
```{r loading}
testing <- read.csv("./pml-testing.csv")
# Test columns that are all NA can be removed from analysis
i_col_Remove <- colSums(is.na(testing)) == NROW(testing)
# Remove time-related fields as each row is a single instance in time
i_col_Remove <- i_col_Remove | grepl("X|time|num_window", colnames(testing))
testing <- testing[,!i_col_Remove]

training <- read.csv("./pml-training.csv")
# Remove these columns from the testing data. 
# If not present in the testing data, they'll have no predictive value
training <- training[,!i_col_Remove]

# Remove all rows where new_window=yes. These are summary data items from a
# repetition of the exercise
training <- training[!training$new_window=="yes",]
```

Partition the training data into three separate groups.  

* *trainData*: data to build our models  
* *testData*: data to test the models  
* *validateData*: data to test the voting system   

*testing* is the final test data for submission
```{r}
set.seed(13163)

# Partition out the test data, 20% of all data
inBuild <- createDataPartition(y=training$classe, p=0.8, list=FALSE)
buildData <- training[inBuild,]
testData <- training[-inBuild,]

# Partition out the training data, 80% of build data
# Partition out the validation data, 20% of build data
inTrain <- createDataPartition(y=buildData$classe, p=0.8, list=FALSE)
trainData <- buildData[inTrain,]
validateData <- buildData[-inTrain,]


# Look for variables with no variation in the training data and eliminate in all sets:
nzv <- nearZeroVar(trainData, saveMetrics = TRUE)
trainData <- trainData[, !(nzv$zeroVar & nzv$nzv)]
testData <- testData[, !(nzv$zeroVar & nzv$nzv)]
validateData  <- validateData[, !(nzv$zeroVar & nzv$nzv)]
```

Train various categorical models using *trainData*. Calculate theit accuracy on *trainData* (in-sample accuracy) and *testData* (out-of-sample accuracy). OOS Accuracy is expected to be lower than the in-sample error.  

Random Forest model
```{r eval=FALSE}
set.seed(13163)
rf.model <- train(classe ~ ., data=trainData, method='rf')
rf.accuracy.train <- confusionMatrix(predict(rf.model, trainData), trainData$classe)$overall[1]
rf.accuracy.test <- confusionMatrix(predict(rf.model, testData), testData$classe)$overall[1]
```

Naive Bayes Model - performed poorly!
```{r eval=FALSE}
set.seed(13163)
nb.model <- train(classe ~ ., data=trainData, method='nb')
nb.accuracy.train <- confusionMatrix(predict(nb.model, trainData), trainData$classe)$overall[1]
nb.accuracy.test <- confusionMatrix(predict(nb.model, testData), testData$classe)$overall[1]
```

Generalized Boosted Regression Model
```{r eval=FALSE}
set.seed(13163)
gbm.model <- train(classe ~ ., data=trainData, method='gbm')
gbm.accuracy.train <- confusionMatrix(predict(gbm.model, trainData), trainData$classe)$overall[1]
gbm.accuracy.test <- confusionMatrix(predict(gbm.model, testData), testData$classe)$overall[1]
```

Linear Discriminate Analysis Model
```{r eval=FALSE}
set.seed(13163)
lda.model <- train(classe ~ ., data=trainData, method='lda')
lda.accuracy.train <- confusionMatrix(predict(lda.model, trainData), trainData$classe)$overall[1]
lda.accuracy.test <- confusionMatrix(predict(lda.model, testData), testData$classe)$overall[1]
```

Generalized Additive Model using Splines
```{r eval=FALSE}
set.seed(13163)
gam.model <- train(classe ~ ., data=trainData, method='gam')
gam.accuracy.train <- confusionMatrix(predict(gam.model, trainData), trainData$classe)$overall[1]
gam.accuracy.test <- confusionMatrix(predict(gam.model, testData), testData$classe)$overall[1]
```

CART Model
```{r eval=FALSE}
set.seed(13163)
rpart.model <- train(classe ~ ., data=trainData, method='rpart')
rpart.accuracy.train <- confusionMatrix(predict(rpart.model, trainData), trainData$classe)$overall[1]
rpart.accuracy.test <- confusionMatrix(predict(rpart.model, testData), testData$classe)$overall[1]
```

K-Nearest Neighbors Model
```{r eval=FALSE}
set.seed(13163)
knn.model <- train(classe ~ ., data=trainData, method='knn')
knn.accuracy.train <- confusionMatrix(predict(knn.model, trainData), trainData$classe)$overall[1]
knn.accuracy.test <- confusionMatrix(predict(knn.model, testData), testData$classe)$overall[1]
```

SVM Model
```{r eval=FALSE}
set.seed(13163)
svm.model <- svm(classe ~ ., data=trainData)
svm.accuracy.train <- confusionMatrix(predict(svm.model, trainData), trainData$classe)$overall[1]
svm.accuracy.test <- confusionMatrix(predict(svm.model, testData), testData$classe)$overall[1]
```

AdaBag Model
```{r eval=FALSE}
set.seed(13163)
adabag.model <- train(classe ~ ., data=trainData, method='AdaBag')
adabag.accuracy.train <- confusionMatrix(predict(adabag.model, trainData), trainData$classe)$overall[1]
adabag.accuracy.test <- confusionMatrix(predict(adabag.model, testData), testData$classe)$overall[1]
```

C5.0 Model
```{r eval=FALSE}
set.seed(13163)
c5.0.model <- train(classe ~ ., data=trainData, method='C5.0')
c5.0.accuracy.train <- confusionMatrix(predict(c5.0.model, trainData), trainData$classe)$overall[1]
c5.0.accuracy.test <- confusionMatrix(predict(c5.0.model, testData), testData$classe)$overall[1]
```

These models were also tried but failed for various reasons:  

* adaboost - did not work  
* bayesglm - only assigned two possible outcomes  
* glm - only works with binary outcomes  
* bag - did not work. Warnings want bagControl passed in  
* rpart - performed poorly  
* adabag - performed poorly  

## Variable Importance  
An attempt to further reduce the number of predictors was undertaken by examining
the variable importance reported by some of the models. It appears that there
are many important variables and little agreement on their level of importance. 
The C5.0 model in particular found every predictor to be important. Seeing this,
I decided to embark on the voting model paradigm.  


| Predictor               |  c5.0  |   gbm  |   rf  
| ----------------------- | ------:| ------:| ------:  
| roll_belt               | 100.00 | 100.00 | 100.00  
| pitch_forearm           |  98.98 |  52.65 |  59.88  
| yaw\_belt               | 100.00 |  42.47 |  51.44  
| pitch\_belt             |  99.47 |  14.21 |  46.67  
| magnet\_dumbbell\_z     |  99.73 |  39.81 |  45.58  
| magnet\_dumbbell\_y     |  93.69 |  25.51 |  44.64  
| roll\_forearm           |   ---  |  21.51 |  39.04  
| accel\_dumbbell\_y      |   ---  |  14.33 |  21.67  
| roll\_dumbbell          |   ---  |  10.01 |  18.36  
| magnet\_dumbbell\_x     |  94.63 |   9.48 |  17.29  
| accel\_forearm\_x       |   ---  |  12.36 |  17.21  
| magnet\_belt\_z         |  99.90 |  17.62 |  16.13  
| accel\_belt\_z          | 100.00 |   ---  |  15.86  
| total\_accel\_dumbbell  |   ---  |   ---  |  15.43  
| magnet\_forearm\_z      |   ---  |  10.29 |  14.86  
| accel\_dumbbell\_z      |   ---  |   ---  |  14.64  
| magnet\_belt\_y         | 100.00 |   6.80 |  13.27  
| yaw\_arm                |  98.65 |   6.50 |  12.04  
| gyros\_belt\_z          | 100.00 |  14.44 |  12.02  
| magnet\_belt\_x         |   ---  |   ---  |  11.34  
| magnet\_forearm\_x      | 100.00 |   ---  |   ---  
| gyros\_belt\_y          |  99.91 |   ---  |   ---  
| magnet\_arm\_z          |  99.70 |   7.02 |   ---  
| magnet\_forearm\_y      |  99.58 |   ---  |   ---  
| roll\_arm               |  98.90 |   ---  |   ---  
| accel\_dumbbell\_x      |  97.94 |   5.89 |   ---  
| gyros\_dumbbell\_y      |  97.84 |   9.52 |   ---  
| accel\_forearm\_x       |  92.99 |   ---  |   ---  
| accel\_forearm\_z       |   ---  |   5.61 |   ---

Here are the training set and out of sample (OOS) accuracies for each of the models.
The top 6 performers in OOS are chosen to continue and be used in the voting system.  

Model  | Training | OOS    | Comments
------ | --------:| ------:| ---------
adabag | 0.4324   | 0.4288 | not used
c5.0   | 1.0000   | 0.9961 |
gam    |          |        |
gbm    | 0.9749   | 0.9599 |
knn    | 0.9447   | 0.8901 |
lda    | 0.7348   | 0.7357 |
nb     | 0.7400   | 0.7300 | warnings: no predictions for some observations
rf     | 1.0000   | 0.9922 | 
rpart  | 0.4987   | 0.4902 | not used
svm    | 0.9413   | 0.9339 |


The six top-performing models (measured by OOS accuracy) predict *classe* on the validation data set.
```{r eval=FALSE}
# Predictions of six top models on the validation data set
c5.0.pred.validation <- predict(c5.0.model, validateData)
c5.0.accuracy.validation <- sum(c5.0.pred.validation == validateData$classe) / nrow(validateData)

gbm.pred.validation <- predict(gbm.model, validateData)
gbm.accuracy.validation <- sum(gbm.pred.validation == validateData$classe) / nrow(validateData)

knn.pred.validation <- predict(knn.model, validateData)
knn.accuracy.validation <- sum(knn.pred.validation == validateData$classe) / nrow(validateData)

lda.pred.validation <- predict(lda.model, validateData)
lda.accuracy.validation <- sum(lds.pred.validation == validateData$classe) / nrow(validateData)

rf.pred.validation <- predict(rf.model, validateData)
rf.accuracy.validation <- sum(rf.pred.validation == validateData$classe) / nrow(validateData)

svm.pred.validation <- predict(svm.model, validateData)
svm.accuracy.validation <- sum(svm.pred.validation == validateData$classe) / nrow(validateData)

df.all <- data.frame(c5.0=c5.0.pred.validation, 
                     gbm=gbm.pred.validation, 
                     knn=knn.pred.validation,
                     lda=lda.pred.validation,
                     rf=rf.pred.validation,
                     svm=svm.pred.validation)

df.all$vote <- as.factor(apply(df.all, 1, Mode))
vote.accuracy.validation <- sum(df.all$vote == validateData$classe) / nrow(validateData)
```

The voting system is not better than all the individual validation tests,
but perhaps that has more to do with the fact that they're all so high to
begin with. There's not much room for improvement.  

As a test, I did remove the worst-performing of the models below (lda) and
re-ran the voting system. The resulting voting accuracy decreased slightly
indicating that even the worst model provided a useful look at some of the
cases where other models must have disagreed.

Model  | trainData | testData | Validation
------ | --------:| -------:| ----------:
c5.0   | 1.0000   | 0.9961  | 0.9948
gbm    | 0.9749   | 0.9599  | 0.9616
knn    | 0.9447   | 0.8901  | 0.9011
lda    | 0.7348   | 0.7357  | 0.7341
rf     | 1.0000   | 0.9922  | 0.9919
svm    | 0.9413   | 0.9339  | 0.9356
Vote   | ------   | ------  | 0.9876

```{r eval=FALSE}
c5.0.pred.testing <- predict(c5.0.model, testing)
gbm.pred.testing <- predict(gbm.model, testing)
knn.pred.testing <- predict(knn.model, testing)
lda.pred.testing <- predict(lda.model, testing)
rf.pred.testing <- predict(rf.model, testing)
svm.pred.testing <- predict(svm.model, testing)

df.testing <- data.frame(c5.0=c5.0.pred.testing, 
                         gbm=gbm.pred.testing, 
                         knn=knn.pred.testing,
                         lda=lda.pred.testing,
                         rf=rf.pred.testing,
                         svm=svm.pred.testing)

df.testing$vote <- as.factor(apply(df.testing, 1, Mode))
```

## Results

The table below shows the final *classe* predictions for each of the final models. 
Accuracy lists the accuracy compared to the test answers. The last column is the 
submitted answers. Incorrect values are highlighted in **bold**.  

Test     |  c5.0  | gbm    | knn    | lda    |   rf   | svm    |  vote
-------- | ------ | ------ | ------ | ------ | ------ | ------ | ------ 
Accuracy | 1.0000 | 1.0000 | 0.9500 | 0.6500 | 1.0000 | 1.0000 | 1.0000 
1        |    B   | B      |    B   |   B    |  B     | B      | B
2        |    A   | A      |    A   |   A    |  A     | A      | A
3        |    B   | B      |    B   |   B    |  B     | B      | B              
4        |    A   | A      |    A   |   A    |  A     | A      | A              
5        |    A   | A      |    A   |   A    |  A     | A      | A              
6        |    E   | E      |    E   | **C**  |  E     | E      | E              
7        |    D   | D      |    D   |   D    |  D     | D      | D              
8        |    B   | B      |  **E** | **D**  |  B     | B      | B              
9        |    A   | A      |    A   |   A    |  A     | A      | A              
10       |    A   | A      |    A   |   A    |  A     | A      | A              
11       |    B   | B      |    B   | **D**  |  B     | B      | B              
12       |    C   | C      |    C   | **A**  |  C     | C      | C              
13       |    B   | B      |    B   |   B    |  B     | B      | B              
14       |    A   | A      |    A   |   A    |  A     | A      | A              
15       |    E   | E      |    E   | **B**  |  E     | E      | E              
16       |    E   | E      |    E   | **A**  |  E     | E      | E              
17       |    A   | A      |    A   | **C**  |  A     | A      | A              
18       |    B   | B      |    B   |   B    |  B     | B      | B              
19       |    B   | B      |    B   |   B    |  B     | B      | B              
20       |    B   | B      |    B   |   B    |  B     | B      | B

## Summary
The voting system only affects the outcomes when there is substantial 
disagreement in the outcome. Using models that examine the data in different
ways can improve the final result in these situations. With our 20 cases,
there was not an opportunity to see this up close.  
Majority Rules!
